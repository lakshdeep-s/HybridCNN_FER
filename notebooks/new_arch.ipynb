{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787034a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as f\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharpenTransform:\n",
    "    def __init__(self, sharpness_factor=2.0):\n",
    "        self.sharpness_factor = sharpness_factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return f.adjust_sharpness(img, self.sharpness_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdcc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    def __init__(self, img_folder, csv_file, transform=None, skip_nf=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_folder (str): Directory containing the image files.\n",
    "            csv_file (str): Path to CSV annotation file containing image names, labels, and emotion distributions.\n",
    "            transform (callable, optional): Optional torchvision transforms to apply on image.\n",
    "            skip_nf (bool): Whether to exclude \"NF\" (Not Face) images from dataset.\n",
    "            \n",
    "        Notes:\n",
    "            - The CSV must include columns for image name, Usage split (Training, PublicTest, PrivateTest),\n",
    "              emotion count columns matching 'neutral', 'happiness', 'surprise', 'sadness', 'anger', \n",
    "              'disgust', 'fear', 'contempt', 'unknown', and the 'NF' label.\n",
    "            - Images are loaded as grayscale, resized to 224x224, and sharpened with factor 5 by default.\n",
    "            - Label distribution is computed by normalizing the raw emotion counts per sample.\n",
    "        \"\"\"\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load CSV with label distributions and metadata\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Map folder name to 'Usage' column value in CSV to filter dataset split accordingly\n",
    "        usage_map = {\n",
    "            \"FER2013Train\": \"Training\",\n",
    "            \"FER2013Valid\": \"PublicTest\",\n",
    "            \"FER2013Test\": \"PrivateTest\"\n",
    "        }\n",
    "        # Identify which subset (split) to load based on folder name\n",
    "        usage_required = usage_map.get(os.path.basename(img_folder), None)\n",
    "        if usage_required is None:\n",
    "            raise ValueError(f\"Unknown dataset folder: {img_folder}\")\n",
    "        \n",
    "        # Filter the dataset to only include images from the selected split\n",
    "        self.data = self.data[self.data['Usage'] == usage_required].reset_index(drop=True)\n",
    "        \n",
    "        # Optionally exclude images marked as Not Face (NF = 1)\n",
    "        if skip_nf:\n",
    "            self.data = self.data[self.data['NF'] == 0].reset_index(drop=True)\n",
    "        \n",
    "        # Define emotion categories (order must match columns in CSV exactly)\n",
    "        self.emotions = ['neutral','happiness','surprise','sadness','anger','disgust','fear','contempt','unknown']\n",
    "        \n",
    "        # If no transform provided, define default preprocessing:\n",
    "        # Convert to 224x224 grayscale, apply sharpening with factor 5\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "                SharpenTransform(sharpness_factor=5),\n",
    "                transforms.ToTensor(),   # convert PIL Image to tensor after preprocessing\n",
    "                transforms.Normalize(mean=[0.5076], std=[0.2119])\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of valid samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename at index\n",
    "        img_name = self.data.loc[idx, 'Image name']\n",
    "\n",
    "        # Construct full image path\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        \n",
    "        # Load image as grayscale with PIL\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        # Apply preprocessing transforms (resize, sharpen, to tensor)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract raw emotion counts for label distribution\n",
    "        counts = self.data.loc[idx, self.emotions].values.astype(float)\n",
    "        \n",
    "        # Normalize counts to create label distribution (probability vector)\n",
    "        label_distribution = counts / counts.sum()\n",
    "        \n",
    "        # Convert label distribution to PyTorch tensor of floats\n",
    "        label_distribution = torch.tensor(label_distribution, dtype=torch.float32)\n",
    "        \n",
    "        # Return preprocessed image tensor and LDL label distribution\n",
    "        return image, label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b15894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Paths remain the same\n",
    "train_imgs_path = \"../FERPlusData/FER2013Train\"\n",
    "val_imgs_path = \"../FERPlusData/FER2013Valid\"\n",
    "test_imgs_path = \"../FERPlusData/FER2013Test\"\n",
    "data_csv_path = \"../fer2013new.csv\"\n",
    "\n",
    "# Define the sharpening transform once to reuse\n",
    "sharpen = SharpenTransform(sharpness_factor=5.0)\n",
    "\n",
    "# Training dataset and loader with full augmentations\n",
    "train_dataset = FERPlusDataset(\n",
    "    img_folder=train_imgs_path,\n",
    "    csv_file=data_csv_path,\n",
    "    skip_nf=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5),\n",
    "        sharpen,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5076], std=[0.2119])\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Validation dataset and loader with only deterministic preprocessing\n",
    "val_dataset = FERPlusDataset(\n",
    "    img_folder=val_imgs_path,\n",
    "    csv_file=data_csv_path,\n",
    "    skip_nf=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        sharpen,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5076], std=[0.2119])\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv1(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8, spatial_kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(spatial_kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.ca(x)\n",
    "        out = out * self.sa(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea314c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECA Module\n",
    "class ECA(nn.Module):\n",
    "    def __init__(self, in_channels, gamma=2, b=1):\n",
    "        super(ECA, self).__init__()\n",
    "        t = int(abs((torch.log2(torch.tensor(in_channels, dtype=torch.float32)) + b) / gamma))\n",
    "        k_size = t if t % 2 else t + 1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = y.squeeze(-1).transpose(-1, -2)\n",
    "        y = self.conv(y)\n",
    "        y = self.sigmoid(y).transpose(-1, -2).unsqueeze(-1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbefad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dropout=False, p=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=p) if dropout else nn.Identity()\n",
    "\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, 1, bias=False) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f29d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBranchFERTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Branch 1 (fine details - 3x3 kernels)\n",
    "        self.b1_block1 = ResidualBlock(1, 32, kernel_size=3, padding=1)\n",
    "        self.b1_pool1 = nn.MaxPool2d(2)\n",
    "        self.b1_block2 = ResidualBlock(32, 64, kernel_size=3, padding=1)\n",
    "        self.b1_pool2 = nn.MaxPool2d(2)\n",
    "        self.b1_block3 = ResidualBlock(64, 128, kernel_size=3, padding=1, dropout=True, p=0.1)\n",
    "        self.b1_pool3 = nn.MaxPool2d(2)\n",
    "        self.b1_block4 = ResidualBlock(128, 128, kernel_size=3, padding=1, dropout=True, p=0.1)\n",
    "        self.cbam_b1 = CBAM(128, ratio=8)  # Strong attention on branch 1 output (reduction=4)\n",
    "\n",
    "        # Branch 2 (broad details - 5x5 kernels)\n",
    "        self.b2_block1 = ResidualBlock(1, 32, kernel_size=5, padding=2)\n",
    "        self.b2_pool1 = nn.MaxPool2d(2)\n",
    "        self.b2_block2 = ResidualBlock(32, 64, kernel_size=5, padding=2)\n",
    "        self.b2_pool2 = nn.MaxPool2d(2)\n",
    "        self.b2_block3 = ResidualBlock(64, 128, kernel_size=5, padding=2, dropout=True, p=0.1)\n",
    "        self.b2_pool3 = nn.MaxPool2d(2)\n",
    "        self.b2_block4 = ResidualBlock(128, 128, kernel_size=5, padding=2, dropout=True, p=0.1)\n",
    "        self.cbam_b2 = CBAM(128, ratio=8)  # Strong attention on branch 2 output (reduction=4)\n",
    "\n",
    "        # Learnable fusion weights for branches after CBAM\n",
    "        self.w1 = nn.Parameter(torch.tensor(0.5))\n",
    "        self.w2 = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "        # Final CBAM on fused features (milder attention, reduction=16)\n",
    "        self.cbam_fused = CBAM(128, ratio=16)\n",
    "\n",
    "        # Transformer encoder: 2 layers, 2 heads, embedding 128\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128, nhead=2, dim_feedforward=256, dropout=0.2, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "        # Final classification fully connected layer for 9 classes\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Branch 1 forward with pooling\n",
    "        b1 = self.b1_block1(x)\n",
    "        b1 = self.b1_pool1(b1)\n",
    "        b1 = self.b1_block2(b1)\n",
    "        b1 = self.b1_pool2(b1)\n",
    "        b1 = self.b1_block3(b1)\n",
    "        b1 = self.b1_pool3(b1)\n",
    "        b1 = self.b1_block4(b1)\n",
    "        b1 = self.cbam_b1(b1)\n",
    "\n",
    "        # Branch 2 forward with pooling\n",
    "        b2 = self.b2_block1(x)\n",
    "        b2 = self.b2_pool1(b2)\n",
    "        b2 = self.b2_block2(b2)\n",
    "        b2 = self.b2_pool2(b2)\n",
    "        b2 = self.b2_block3(b2)\n",
    "        b2 = self.b2_pool3(b2)\n",
    "        b2 = self.b2_block4(b2)\n",
    "        b2 = self.cbam_b2(b2)\n",
    "\n",
    "        # Weighted fusion of the two branch outputs\n",
    "        fused = self.w1 * b1 + self.w2 * b2\n",
    "\n",
    "        # Final CBAM on fused features (milder attention)\n",
    "        fused = self.cbam_fused(fused)\n",
    "        fused = F.dropout(fused, p=0.2, training=self.training)\n",
    "\n",
    "        # Prepare transformer input: [batch, seq_len, embed_dim]\n",
    "        B, C, H, W = fused.shape\n",
    "        out = fused.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "\n",
    "        # Transformer encoder\n",
    "        out = self.transformer(out)\n",
    "\n",
    "        # Global average pooling over sequence dimension\n",
    "        out = out.mean(dim=1)\n",
    "\n",
    "        # Final classification layer\n",
    "        logits = self.fc(out)\n",
    "\n",
    "        # Softmax for label distribution output\n",
    "        return torch.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea49382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Device\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Max epochs\n",
    "max_epochs = 50\n",
    "\n",
    "# 3. Model & Optimizer\n",
    "model = TwoBranchFERTransformer(num_classes=9).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# 4. LR Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=max_epochs,\n",
    "    eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd903ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class EarlyStopper:\n",
    "    \"\"\"\n",
    "    Early stops training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0.0, verbose=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last improvement before stopping.\n",
    "            min_delta (float): Minimum change to qualify as improvement.\n",
    "            verbose (bool): Print messages when validation improves.\n",
    "            save_path (str or None): Path to save the best model checkpoint.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.save_path is not None:\n",
    "                torch.save(model.state_dict(), self.save_path)\n",
    "                if self.verbose:\n",
    "                    print(f\"Validation loss decreased. Saving model to {self.save_path}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, label_distributions):\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    loss = F.kl_div(log_probs, label_distributions, reduction='batchmean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "from torchmetrics import MeanSquaredError\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_val_loop(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss_fn,\n",
    "    early_stopper,\n",
    "    num_epochs,\n",
    "    device\n",
    "):\n",
    "    # Initialize torchmetrics for accuracy, f1 and mse on the device\n",
    "    train_acc = MulticlassAccuracy(num_classes=9, average='micro').to(device)\n",
    "    val_acc = MulticlassAccuracy(num_classes=9, average='micro').to(device)\n",
    "\n",
    "    train_f1 = MulticlassF1Score(num_classes=9, average='weighted').to(device)\n",
    "    val_f1 = MulticlassF1Score(num_classes=9, average='weighted').to(device)\n",
    "\n",
    "    train_mse = MeanSquaredError().to(device)\n",
    "    val_mse = MeanSquaredError().to(device)\n",
    "\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    use_amp = (device.type == 'cuda')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_acc.reset()\n",
    "        train_f1.reset()\n",
    "        train_mse.reset()\n",
    "        train_samples = 0\n",
    "\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=False)\n",
    "        for images, labels in train_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(enabled=use_amp, device_type=device.type):\n",
    "                logits = model(images)\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_train_loss += loss.item() * images.size(0)\n",
    "            train_samples += images.size(0)\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            targets = torch.argmax(labels, dim=1)\n",
    "\n",
    "            train_acc.update(preds, targets)\n",
    "            train_f1.update(preds, targets)\n",
    "            train_mse.update(probs, labels)\n",
    "\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_train_loss / train_samples\n",
    "        avg_train_acc = train_acc.compute().item()\n",
    "        avg_train_f1 = train_f1.compute().item()\n",
    "        avg_train_mse = train_mse.compute().item()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_acc.reset()\n",
    "        val_f1.reset()\n",
    "        val_mse.reset()\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch} Validation\", leave=False)\n",
    "            for images, labels in val_bar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with torch.amp.autocast(enabled=use_amp, device_type=device.type):\n",
    "                    logits = model(images)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "                total_val_loss += loss.item() * images.size(0)\n",
    "                val_samples += images.size(0)\n",
    "\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                targets = torch.argmax(labels, dim=1)\n",
    "\n",
    "                val_acc.update(preds, targets)\n",
    "                val_f1.update(preds, targets)\n",
    "                val_mse.update(probs, labels)\n",
    "\n",
    "                val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_val_loss = total_val_loss / val_samples\n",
    "        avg_val_acc = val_acc.compute().item()\n",
    "        avg_val_f1 = val_f1.compute().item()\n",
    "        avg_val_mse = val_mse.compute().item()\n",
    "\n",
    "        # Step learning rate scheduler once per epoch after validation\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print training and validation results for the epoch\n",
    "        print(f\"[Epoch {epoch}/{num_epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Train Acc: {avg_train_acc:.4f}, Val Acc: {avg_val_acc:.4f} | \"\n",
    "              f\"Train F1: {avg_train_f1:.4f}, Val F1: {avg_val_f1:.4f} | \"\n",
    "              f\"Train MSE: {avg_train_mse:.4f}, Val MSE: {avg_val_mse:.4f}\")\n",
    "\n",
    "        # Early stopping check and save best model\n",
    "        early_stopper(avg_val_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # Load the best weights saved by early stopper (if any)\n",
    "    if early_stopper.save_path is not None:\n",
    "        print(f\"Loading best model from {early_stopper.save_path}\")\n",
    "        model.load_state_dict(torch.load(early_stopper.save_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    verbose=True,\n",
    "    save_path=\"../models/early_best_models/new_arch_best_model.pth\"\n",
    ")\n",
    "\n",
    "train_val_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=loss_fn,\n",
    "    early_stopper=early_stopper,\n",
    "    num_epochs=max_epochs,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor with the same shape as your model input\n",
    "# For example, if your images are grayscale 224x224 and batch size 1:\n",
    "dummy_input = torch.randn(1, 1, 224, 224, device=device)\n",
    "\n",
    "# Define the path where to save the ONNX model\n",
    "onnx_path = \"../models/new_arch.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,                      # model being run\n",
    "    dummy_input,                # model input (or a tuple for multiple inputs)\n",
    "    onnx_path,                  # where to save the model\n",
    "    export_params=True,         # store the trained parameter weights inside the model file\n",
    "    opset_version=12,           # ONNX version to export the model to\n",
    "    do_constant_folding=True,   # whether to execute constant folding for optimization\n",
    "    input_names=['input'],      # the model's input names\n",
    "    output_names=['output'],    # the model's output names\n",
    "    dynamic_axes={              # variable length axes\n",
    "        'input': {0: 'batch_size'},   # allow variable batch size for input\n",
    "        'output': {0: 'batch_size'}   # allow variable batch size for output\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model has been successfully exported to {onnx_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
