{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d5fed16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as f\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e93977e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharpenTransform:\n",
    "    def __init__(self, sharpness_factor=2.0):\n",
    "        self.sharpness_factor = sharpness_factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return f.adjust_sharpness(img, self.sharpness_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3e057bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERPlusDataset(Dataset):\n",
    "    def __init__(self, img_folder, csv_file, transform=None, skip_nf=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_folder (str): Directory containing the image files.\n",
    "            csv_file (str): Path to CSV annotation file containing image names, labels, and emotion distributions.\n",
    "            transform (callable, optional): Optional torchvision transforms to apply on image.\n",
    "            skip_nf (bool): Whether to exclude \"NF\" (Not Face) images from dataset.\n",
    "            \n",
    "        Notes:\n",
    "            - The CSV must include columns for image name, Usage split (Training, PublicTest, PrivateTest),\n",
    "              emotion count columns matching 'neutral', 'happiness', 'surprise', 'sadness', 'anger', \n",
    "              'disgust', 'fear', 'contempt', 'unknown', and the 'NF' label.\n",
    "            - Images are loaded as grayscale, resized to 224x224, and sharpened with factor 5 by default.\n",
    "            - Label distribution is computed by normalizing the raw emotion counts per sample.\n",
    "        \"\"\"\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load CSV with label distributions and metadata\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Map folder name to 'Usage' column value in CSV to filter dataset split accordingly\n",
    "        usage_map = {\n",
    "            \"FER2013Train\": \"Training\",\n",
    "            \"FER2013Valid\": \"PublicTest\",\n",
    "            \"FER2013Test\": \"PrivateTest\"\n",
    "        }\n",
    "        # Identify which subset (split) to load based on folder name\n",
    "        usage_required = usage_map.get(os.path.basename(img_folder), None)\n",
    "        if usage_required is None:\n",
    "            raise ValueError(f\"Unknown dataset folder: {img_folder}\")\n",
    "        \n",
    "        # Filter the dataset to only include images from the selected split\n",
    "        self.data = self.data[self.data['Usage'] == usage_required].reset_index(drop=True)\n",
    "        \n",
    "        # Optionally exclude images marked as Not Face (NF = 1)\n",
    "        if skip_nf:\n",
    "            self.data = self.data[self.data['NF'] == 0].reset_index(drop=True)\n",
    "        \n",
    "        # Define emotion categories (order must match columns in CSV exactly)\n",
    "        self.emotions = ['neutral','happiness','surprise','sadness','anger','disgust','fear','contempt','unknown']\n",
    "        \n",
    "        # If no transform provided, define default preprocessing:\n",
    "        # Convert to 224x224 grayscale, apply sharpening with factor 5\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((112, 112), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "                SharpenTransform(sharpness_factor=2.0),\n",
    "                transforms.ToTensor(),   # convert PIL Image to tensor after preprocessing\n",
    "                transforms.Normalize(mean=[0.5076], std=[0.2119])\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of valid samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename at index\n",
    "        img_name = self.data.loc[idx, 'Image name']\n",
    "\n",
    "        # Construct full image path\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        \n",
    "        # Load image as grayscale with PIL\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        # Apply preprocessing transforms (resize, sharpen, to tensor)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Extract raw emotion counts for label distribution\n",
    "        counts = self.data.loc[idx, self.emotions].values.astype(float)\n",
    "        \n",
    "        # Normalize counts to create label distribution (probability vector)\n",
    "        label_distribution = counts / counts.sum()\n",
    "        \n",
    "        # Convert label distribution to PyTorch tensor of floats\n",
    "        label_distribution = torch.tensor(label_distribution, dtype=torch.float32)\n",
    "        \n",
    "        # Return preprocessed image tensor and LDL label distribution\n",
    "        return image, label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f733510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Paths remain the same\n",
    "train_imgs_path = \"../FERPlusData/FER2013Train\"\n",
    "val_imgs_path = \"../FERPlusData/FER2013Valid\"\n",
    "test_imgs_path = \"../FERPlusData/FER2013Test\"\n",
    "data_csv_path = \"../fer2013new.csv\"\n",
    "\n",
    "# Define the sharpening transform once to reuse\n",
    "sharpen = SharpenTransform(sharpness_factor=2.0)\n",
    "\n",
    "# Training dataset and loader with full augmentations\n",
    "train_dataset = FERPlusDataset(\n",
    "    img_folder=train_imgs_path,\n",
    "    csv_file=data_csv_path,\n",
    "    skip_nf=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((112, 112), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5),\n",
    "        sharpen,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5076], std=[0.2119])\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Validation dataset and loader with only deterministic preprocessing\n",
    "val_dataset = FERPlusDataset(\n",
    "    img_folder=val_imgs_path,\n",
    "    csv_file=data_csv_path,\n",
    "    skip_nf=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((112, 112), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        sharpen,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5076], std=[0.2119])\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e2d6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1=nn.Conv2d(in_planes, in_planes//ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "    \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv1(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8, spatial_kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(spatial_kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.ca(x)\n",
    "        out = out * self.sa(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ec1d939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dropout=False, p=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=p) if dropout else nn.Identity()\n",
    "\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, 1, bias=False) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bc70e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCBAMClassifier(nn.Module):\n",
    "    def __init__(self, img_size=112, in_channels=1, num_classes=9):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, 16),\n",
    "            ResidualBlock(16, 32),\n",
    "            ResidualBlock(32, 64),\n",
    "            ResidualBlock(64, 128),\n",
    "            ResidualBlock(128, 128, dropout=True, p=0.1)\n",
    "        )\n",
    "        self.cbam = CBAM(128, ratio=8)\n",
    "\n",
    "        # Get flattened feature dim dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, img_size, img_size)\n",
    "            feat_dim = torch.flatten(self.cbam(self.blocks(dummy)), 1).shape[1]\n",
    "\n",
    "        self.fc = nn.Linear(feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.cbam(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.fc(x)\n",
    "        return F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f69e9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. Device\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Max epochs\n",
    "max_epochs = 25\n",
    "\n",
    "# 3. Model & Optimizer\n",
    "model = ResidualCBAMClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# 4. LR Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=max_epochs,\n",
    "    eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "333fa325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class EarlyStopper:\n",
    "    \"\"\"\n",
    "    Early stops training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0.0, verbose=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last improvement before stopping.\n",
    "            min_delta (float): Minimum change to qualify as improvement.\n",
    "            verbose (bool): Print messages when validation improves.\n",
    "            save_path (str or None): Path to save the best model checkpoint.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.save_path is not None:\n",
    "                torch.save(model.state_dict(), self.save_path)\n",
    "                if self.verbose:\n",
    "                    print(f\"Validation loss decreased. Saving model to {self.save_path}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cd2b6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, label_distributions):\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    loss = F.kl_div(log_probs, label_distributions, reduction='batchmean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "28268e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "from torchmetrics import MeanSquaredError\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_val_loop(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss_fn,\n",
    "    early_stopper,\n",
    "    num_epochs,\n",
    "    device\n",
    "):\n",
    "    # Initialize torchmetrics for accuracy, f1 and mse on the device\n",
    "    train_acc = MulticlassAccuracy(num_classes=9, average='micro').to(device)\n",
    "    val_acc = MulticlassAccuracy(num_classes=9, average='micro').to(device)\n",
    "\n",
    "    train_f1 = MulticlassF1Score(num_classes=9, average='weighted').to(device)\n",
    "    val_f1 = MulticlassF1Score(num_classes=9, average='weighted').to(device)\n",
    "\n",
    "    train_mse = MeanSquaredError().to(device)\n",
    "    val_mse = MeanSquaredError().to(device)\n",
    "\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    use_amp = (device.type == 'cuda')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_acc.reset()\n",
    "        train_f1.reset()\n",
    "        train_mse.reset()\n",
    "        train_samples = 0\n",
    "\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=False)\n",
    "        for images, labels in train_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(enabled=use_amp, device_type=device.type):\n",
    "                logits = model(images)\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_train_loss += loss.item() * images.size(0)\n",
    "            train_samples += images.size(0)\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            targets = torch.argmax(labels, dim=1)\n",
    "\n",
    "            train_acc.update(preds, targets)\n",
    "            train_f1.update(preds, targets)\n",
    "            train_mse.update(probs, labels)\n",
    "\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_train_loss / train_samples\n",
    "        avg_train_acc = train_acc.compute().item()\n",
    "        avg_train_f1 = train_f1.compute().item()\n",
    "        avg_train_mse = train_mse.compute().item()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_acc.reset()\n",
    "        val_f1.reset()\n",
    "        val_mse.reset()\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch} Validation\", leave=False)\n",
    "            for images, labels in val_bar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with torch.amp.autocast(enabled=use_amp, device_type=device.type):\n",
    "                    logits = model(images)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "                total_val_loss += loss.item() * images.size(0)\n",
    "                val_samples += images.size(0)\n",
    "\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                targets = torch.argmax(labels, dim=1)\n",
    "\n",
    "                val_acc.update(preds, targets)\n",
    "                val_f1.update(preds, targets)\n",
    "                val_mse.update(probs, labels)\n",
    "\n",
    "                val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_val_loss = total_val_loss / val_samples\n",
    "        avg_val_acc = val_acc.compute().item()\n",
    "        avg_val_f1 = val_f1.compute().item()\n",
    "        avg_val_mse = val_mse.compute().item()\n",
    "\n",
    "        # Step learning rate scheduler once per epoch after validation\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print training and validation results for the epoch\n",
    "        print(f\"[Epoch {epoch}/{num_epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Train Acc: {avg_train_acc:.4f}, Val Acc: {avg_val_acc:.4f} | \"\n",
    "              f\"Train F1: {avg_train_f1:.4f}, Val F1: {avg_val_f1:.4f} | \"\n",
    "              f\"Train MSE: {avg_train_mse:.4f}, Val MSE: {avg_val_mse:.4f}\")\n",
    "\n",
    "        # Early stopping check and save best model\n",
    "        early_stopper(avg_val_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # Load the best weights saved by early stopper (if any)\n",
    "    if early_stopper.save_path is not None:\n",
    "        print(f\"Loading best model from {early_stopper.save_path}\")\n",
    "        model.load_state_dict(torch.load(early_stopper.save_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b1e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/890 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/25] Train Loss: 1.4384, Val Loss: 1.4919 | Train Acc: 0.3603, Val Acc: 0.3727 | Train F1: 0.1914, Val F1: 0.2023 | Train MSE: 0.0550, Val MSE: 0.0587\n",
      "Validation loss decreased. Saving model to ../models/early_best_models/new_arch_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/25] Train Loss: 1.4383, Val Loss: 1.4919 | Train Acc: 0.3607, Val Acc: 0.3727 | Train F1: 0.1912, Val F1: 0.2023 | Train MSE: 0.0550, Val MSE: 0.0587\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/25] Train Loss: 1.4383, Val Loss: 1.4919 | Train Acc: 0.3607, Val Acc: 0.3727 | Train F1: 0.1912, Val F1: 0.2023 | Train MSE: 0.0550, Val MSE: 0.0587\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m early_stopper = EarlyStopper(\n\u001b[32m      2\u001b[39m     patience=\u001b[32m10\u001b[39m,\n\u001b[32m      3\u001b[39m     min_delta=\u001b[32m1e-4\u001b[39m,\n\u001b[32m      4\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m     save_path=\u001b[33m\"\u001b[39m\u001b[33m../models/early_best_models/new_arch_best_model.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrain_val_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[141]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtrain_val_loop\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, scheduler, loss_fn, early_stopper, num_epochs, device)\u001b[39m\n\u001b[32m     44\u001b[39m     logits = model(images)\n\u001b[32m     45\u001b[39m     loss = loss_fn(logits, labels)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m scaler.step(optimizer)\n\u001b[32m     49\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "early_stopper = EarlyStopper(\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    verbose=True,\n",
    "    save_path=\"../models/early_best_models/new_arch_best_model.pth\"\n",
    ")\n",
    "\n",
    "train_val_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=loss_fn,\n",
    "    early_stopper=early_stopper,\n",
    "    num_epochs=max_epochs,\n",
    "    device=device\n",
    ")\n",
    "# We will need to change the model or aq\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
