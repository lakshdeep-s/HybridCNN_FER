{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a117c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "00860044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)                   # Python random module\n",
    "    np.random.seed(seed)                # NumPy random seed\n",
    "    torch.manual_seed(seed)             # PyTorch CPU seed\n",
    "    torch.cuda.manual_seed(seed)        # PyTorch CUDA seed\n",
    "    torch.cuda.manual_seed_all(seed)    # All CUDA devices if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True   # For reproducibility\n",
    "    torch.backends.cudnn.benchmark = False      # Disable for reproducibility\n",
    "\n",
    "# Example usage:\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "acf8280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mean = [0.54148953, 0.42486119, 0.37428667]\n",
    "dataset_std = [0.23021227, 0.2072772, 0.1976669 ]\n",
    "\n",
    "class ExpDataset(Dataset):\n",
    "    def __init__(self, imgs_path, csv_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.imgs_path = imgs_path\n",
    "        self.csv_data = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "    def __getitem__(self, index):\n",
    "        record = self.csv_data.iloc[index]\n",
    "\n",
    "        img_name = record['image_name']\n",
    "        img_path = os.path.join(self.imgs_path, img_name+\".jpg\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transforms to the image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.Compose([\n",
    "                transforms.Resize((224, 224)).interpolation,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "            ])(img)\n",
    "        \n",
    "        label = record['expression_label']\n",
    "        return img, label\n",
    "\n",
    "imgs_path = \"../expW/origin_cleaned\"\n",
    "labels_path = \"../expW/new_label.csv\"\n",
    "\n",
    "exp_dataset = ExpDataset(imgs_path, labels_path)\n",
    "N = len(exp_dataset)\n",
    "dataset_range = range(N)\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    dataset_range,\n",
    "    train_size=0.8,\n",
    "    random_state=42,\n",
    "    stratify=exp_dataset.csv_data['expression_label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b7ed8b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3521, -2.1307, -2.1307,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.1307, -2.1307,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.1307, -2.1307,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         ...,\n",
       "         [-2.3521, -2.3521, -2.3521,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.3521, -2.3521,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.3521, -2.3521,  ..., -2.1307, -2.1307, -2.1307]],\n",
       "\n",
       "        [[-2.0497, -1.8038, -1.8038,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -1.8038, -1.8038,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -1.8038, -1.8038,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         ...,\n",
       "         [-2.0497, -2.0497, -2.0497,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -2.0497, -2.0497,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -2.0497, -2.0497,  ..., -1.8038, -1.8038, -1.8038]],\n",
       "\n",
       "        [[-1.8935, -1.6356, -1.6356,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.6356, -1.6356,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.6356, -1.6356,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         ...,\n",
       "         [-1.8935, -1.8935, -1.8935,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.8935, -1.8935,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.8935, -1.8935,  ..., -1.6356, -1.6356, -1.6356]]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.25),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=2.5, translate=(0.05, 0.05), scale=(1.05, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])\n",
    "\n",
    "train_dataset = ExpDataset(imgs_path, labels_path, transform=train_transforms)\n",
    "val_dataset = ExpDataset(imgs_path, labels_path, transform=val_transforms)\n",
    "\n",
    "# Final Splits\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "img, label = train_dataset[36]\n",
    "img\n",
    "# len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2d8a2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "165ac8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, padding=1, dropout=False, d=0.05, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Reduce Spatial Dimenstion here by half or more by taking (stride > 1)\n",
    "        # Use the kernel size to determine what 'kind' of features to focus on, smaller kernels for finer detail but needs more stacks\n",
    "        # to achieve the perfect receptive field\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout1 = nn.Dropout2d(d) if dropout else nn.Identity()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Use stride 1 here always\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            stride=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout2 = nn.Dropout2d(d) if dropout else nn.Identity()\n",
    "\n",
    "        # Skip connection Logic\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,  # match spatial downsample\n",
    "                    padding=0,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out_intermediate = self.relu(self.bn1(self.conv1(x)))\n",
    "        out_intermediate = self.dropout1(out_intermediate)\n",
    "\n",
    "        out_intermediate = self.bn2(self.conv2(out_intermediate))\n",
    "        out_intermediate = self.dropout2(out_intermediate)\n",
    "\n",
    "        out = out_intermediate + self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        return out, out_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b3f02d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1=nn.Conv2d(in_planes, in_planes//ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "    \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv1(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8, spatial_kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(spatial_kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.ca(x)\n",
    "        out = out * self.sa(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "11bb3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branch1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = ResidualBlock(16, 16, stride=1, kernel_size=3, padding=1)\n",
    "        self.block2 = ResidualBlock(16, 32, stride=2, kernel_size=3, padding=1)\n",
    "        self.cbam1 = CBAM(32, ratio=4)\n",
    "        self.block3 = ResidualBlock(32, 64, stride=2, kernel_size=3, padding=1)\n",
    "        self.cbam2 = CBAM(64, ratio=8)\n",
    "        self.block4 = ResidualBlock(64, 128, stride=2, kernel_size=3, padding=1, dropout=True)\n",
    "        self.cbam3 = CBAM(128, ratio=8)\n",
    "        self.block5 = ResidualBlock(128, 128, stride=1, kernel_size=3, padding=1, dropout=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.block1(x)\n",
    "        out, _ = self.block2(out)\n",
    "        out = self.cbam1(out)\n",
    "        out, _ = self.block3(out)\n",
    "        out = self.cbam2(out)\n",
    "        out, _ = self.block4(out)\n",
    "        out = self.cbam3(out)\n",
    "        out, final_intermediate = self.block5(out)\n",
    "        return out, final_intermediate\n",
    "\n",
    "class Branch2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = ResidualBlock(16, 32, stride=2, kernel_size=5, padding=2)\n",
    "        self.cbam1 = CBAM(32, ratio=4)\n",
    "        self.block2 = ResidualBlock(32, 64, stride=2, kernel_size=5, padding=2, dropout=True)\n",
    "        self.cbam2 = CBAM(64, ratio=8)\n",
    "        self.block3 = ResidualBlock(64, 128, stride=2, kernel_size=5, padding=2, dropout=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.block1(x)\n",
    "        out = self.cbam1(out)\n",
    "        out, _ = self.block2(out)\n",
    "        out = self.cbam2(out)\n",
    "        out, final_intermediate = self.block3(out)\n",
    "        return out, final_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5c883138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MildCBAM(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super().__init__()\n",
    "        self.cbam = CBAM(in_planes, ratio=16)  # mild attention\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn = self.cbam(x)\n",
    "        return x * attn\n",
    "\n",
    "class AttentionWeightedConcat(nn.Module):\n",
    "    def __init__(self, channels1, channels2):\n",
    "        super().__init__()\n",
    "        self.attn1 = MildCBAM(channels1)\n",
    "        self.attn2 = MildCBAM(channels2)\n",
    "        self.weight1 = nn.Parameter(torch.ones(1, channels1, 1, 1))\n",
    "        self.weight2 = nn.Parameter(torch.ones(1, channels2, 1, 1))\n",
    "        self.batch_norm = nn.BatchNorm2d(channels1 + channels2)\n",
    "    \n",
    "    def forward(self, feat1, feat2):\n",
    "        modulated1 = self.attn1(feat1)\n",
    "        modulated2 = self.attn2(feat2)\n",
    "        weighted1 = modulated1 * self.weight1\n",
    "        weighted2 = modulated2 * self.weight2\n",
    "        fused = torch.cat([weighted1, weighted2], dim=1)\n",
    "        fused = self.batch_norm(fused)\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ddb37256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBranchModel(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.branch1 = Branch1()  # Your previously defined branch1\n",
    "        self.branch2 = Branch2()  # Your previously defined branch2\n",
    "        self.fusion = AttentionWeightedConcat(128, 128)  # 128 channels from each branch\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared = self.initial(x)\n",
    "        b1_out, b1_intermediate = self.branch1(shared)\n",
    "        b2_out, b2_intermediate = self.branch2(shared)\n",
    "\n",
    "        if b1_out.shape[2:] != b2_out.shape[2:]:\n",
    "            b2_out = nn.functional.interpolate(b2_out, size=b1_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        fused = self.fusion(b1_out, b2_out)\n",
    "\n",
    "        return fused, b1_intermediate, b2_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fc59c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch1 intermediate: torch.Size([1, 128, 14, 14])\n",
      "Branch2 intermediate: torch.Size([1, 128, 14, 14])\n",
      "Fused features: torch.Size([1, 256, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "model = ResnetBranchModel()\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "fused, b1_int, b2_int = model(x)\n",
    "\n",
    "print(f\"Branch1 intermediate: {b1_int.shape}\")\n",
    "print(f\"Branch2 intermediate: {b2_int.shape}\")\n",
    "print(f\"Fused features: {fused.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ca17d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_channels_fused, input_channels_int, embed_dim=256, num_heads=8, num_layers=4, height=14, width=14):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Project each input (fused and intermediates) to a common embedding dimension\n",
    "        self.proj_fused = nn.Conv2d(input_channels_fused, embed_dim, kernel_size=1)\n",
    "        self.proj_b1_int = nn.Conv2d(input_channels_int, embed_dim, kernel_size=1)\n",
    "        self.proj_b2_int = nn.Conv2d(input_channels_int, embed_dim, kernel_size=1)\n",
    "\n",
    "        # Learnable positional embeddings for spatial information\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_dim, height, width))\n",
    "\n",
    "        # Learnable path embeddings to distinguish between fused and branch inputs\n",
    "        self.path_embed = nn.Parameter(torch.randn(3, 1, embed_dim))  # for fused, branch1 intermediate, branch2 intermediate\n",
    "\n",
    "        # Transformer encoder stack with multi-head attention\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=0.1,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, fused, b1_int, b2_int):\n",
    "        B = fused.size(0)\n",
    "\n",
    "        # Project inputs and add positional embedding\n",
    "        fused_embed = self.proj_fused(fused) + self.pos_embed\n",
    "        b1_embed = self.proj_b1_int(b1_int) + self.pos_embed\n",
    "        b2_embed = self.proj_b2_int(b2_int) + self.pos_embed\n",
    "\n",
    "        # Flatten spatial dimensions (B, C, H, W) -> (B, H*W, C)\n",
    "        def flatten_spatial(x):\n",
    "            B, C, H, W = x.shape\n",
    "            return x.flatten(2).permute(0, 2, 1)  # shape: (B, H*W, C)\n",
    "\n",
    "        fused_seq = flatten_spatial(fused_embed)\n",
    "        b1_seq = flatten_spatial(b1_embed)\n",
    "        b2_seq = flatten_spatial(b2_embed)\n",
    "\n",
    "        # Add path embeddings broadcasted across batch and sequence length\n",
    "        fused_seq = fused_seq + self.path_embed[0]\n",
    "        b1_seq = b1_seq + self.path_embed[1]\n",
    "        b2_seq = b2_seq + self.path_embed[2]\n",
    "\n",
    "        # Concatenate sequences along token dimension (B, 3*H*W, embed_dim)\n",
    "        combined_seq = torch.cat([fused_seq, b1_seq, b2_seq], dim=1)\n",
    "\n",
    "        # Transformer expects input shape (sequence length, batch, embedding dim)\n",
    "        combined_seq = combined_seq.permute(1, 0, 2)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        transformed_seq = self.transformer_encoder(combined_seq)\n",
    "\n",
    "        # Back to (batch, sequence length, embedding dim)\n",
    "        transformed_seq = transformed_seq.permute(1, 0, 2)\n",
    "\n",
    "        # Layer norm on transformer output\n",
    "        transformed_seq = self.layer_norm(transformed_seq)\n",
    "\n",
    "        return transformed_seq  # Shape: (B, 3*H*W, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c78c98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBranchHybridTransformer(L.LightningModule):\n",
    "    def __init__(self, backbone, transformer, num_classes=7,\n",
    "                 lr=1e-3, weight_decay=1e-4, max_epochs=40, min_lr=1e-6):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['backbone', 'transformer'])\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.transformer = transformer\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Set classifier input size from transformer params\n",
    "        embed_dim = transformer.layer_norm.normalized_shape[0]\n",
    "        H, W = transformer.height, transformer.width\n",
    "        self.classifier = nn.Linear(embed_dim * 3 * H * W, num_classes)\n",
    "\n",
    "        # Metrics\n",
    "        self.train_acc = MulticlassAccuracy(num_classes=num_classes, average='weighted')\n",
    "        self.val_acc = MulticlassAccuracy(num_classes=num_classes, average='weighted')\n",
    "        self.train_f1 = MulticlassF1Score(num_classes=num_classes, average='weighted')\n",
    "        self.val_f1 = MulticlassF1Score(num_classes=num_classes, average='weighted')\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        fused, b1_int, b2_int = self.backbone(x)\n",
    "        transformer_out = self.transformer(fused, b1_int, b2_int)  # [B, 3*H*W, embed_dim]\n",
    "        B = transformer_out.size(0)\n",
    "        logits = self.classifier(transformer_out.view(B, -1))\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.train_f1.update(preds, y)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = self.trainer.callback_metrics.get('train_loss')\n",
    "        acc = self.train_acc.compute().item()\n",
    "        f1 = self.train_f1.compute().item()\n",
    "\n",
    "        print(f\"[Epoch {self.current_epoch}] \"\n",
    "              f\"Train Loss: {avg_loss:.4f} | \"\n",
    "              f\"Train Acc: {acc:.4f} | \"\n",
    "              f\"Train F1: {f1:.4f}\")\n",
    "\n",
    "        self.train_acc.reset()\n",
    "        self.train_f1.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc.update(preds, y)\n",
    "        self.val_f1.update(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = self.trainer.callback_metrics.get('val_loss')\n",
    "        acc = self.val_acc.compute().item()\n",
    "        f1 = self.val_f1.compute().item()\n",
    "\n",
    "        print(f\"[Epoch {self.current_epoch}] \"\n",
    "              f\"Val Loss: {avg_loss:.4f} | \"\n",
    "              f\"Val Acc: {acc:.4f} | \"\n",
    "              f\"Val F1: {f1:.4f}\")\n",
    "\n",
    "        self.val_acc.reset()\n",
    "        self.val_f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.hparams.max_epochs,\n",
    "            eta_min=self.hparams.min_lr\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            'lr_scheduler': {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a88df69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "backbone = ResnetBranchModel(in_channels=3)\n",
    "# Instantiate the MultiInputTransformerEncoder\n",
    "# Input channel details from your model:\n",
    "# - fused features channels = 256 (128 from each branch concatenated)\n",
    "# - intermediate features channels = 128 (each branch outputs 128 channels intermediate)\n",
    "transformer = MultiInputTransformerEncoder(\n",
    "    input_channels_fused=256,\n",
    "    input_channels_int=128,\n",
    "    embed_dim=256,       # embedding dimension for transformer tokens\n",
    "    num_heads=8,         # number of attention heads\n",
    "    num_layers=4,        # number of transformer encoder layers\n",
    "    height=14,           # spatial height of feature maps\n",
    "    width=14             # spatial width of feature maps\n",
    ")\n",
    "model = ResnetBranchHybridTransformer(\n",
    "    backbone=backbone,\n",
    "    transformer=transformer,\n",
    "    num_classes=7\n",
    ")\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_top_k=1,\n",
    "    dirpath='../models/checkpoints_',\n",
    "    filename='best_model'\n",
    ")\n",
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=40,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    log_every_n_steps=1,\n",
    "    logger=False,\n",
    "    devices=1,\n",
    "    precision='16-mixed',\n",
    "    num_sanity_val_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a1278276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                         | Params | Mode \n",
      "---------------------------------------------------------------------\n",
      "0 | backbone    | ResnetBranchModel            | 1.4 M  | train\n",
      "1 | transformer | MultiInputTransformerEncoder | 3.3 M  | train\n",
      "2 | classifier  | Linear                       | 1.1 M  | train\n",
      "3 | train_acc   | MulticlassAccuracy           | 0      | train\n",
      "4 | val_acc     | MulticlassAccuracy           | 0      | train\n",
      "5 | train_f1    | MulticlassF1Score            | 0      | train\n",
      "6 | val_f1      | MulticlassF1Score            | 0      | train\n",
      "7 | loss_fn     | CrossEntropyLoss             | 0      | train\n",
      "---------------------------------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.329    Total estimated model params size (MB)\n",
      "225       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2231702b44e4edab51581b5de4582a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Val Loss: 2.0134 | Val Acc: 0.4062 | Val F1: 0.2347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3be6bee099846dc94f7706a36d00f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:306\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    305\u001b[39m dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m batch, _, __ = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:134\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     batch = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:61\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:78\u001b[39m, in \u001b[36m_MaxSizeCycle.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     out[i] = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[193]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mExpDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1280\u001b[39m, in \u001b[36mColorJitter.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1279\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1280\u001b[39m         img = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:968\u001b[39m, in \u001b[36madjust_hue\u001b[39m\u001b[34m(img, hue_factor)\u001b[39m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.adjust_hue(img, hue_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:109\u001b[39m, in \u001b[36madjust_hue\u001b[39m\u001b[34m(img, hue_factor)\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m h, s, v = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHSV\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.split()\n\u001b[32m    111\u001b[39m np_h = np.array(h, dtype=np.uint8)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\PIL\\Image.py:1145\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1145\u001b[39m     im = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[205]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_cb.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "best_model = ResnetBranchHybridTransformer.load_from_checkpoint(\n",
    "    best_model_path,\n",
    "    backbone=backbone,\n",
    "    transformer=transformer,\n",
    "    num_classes=7\n",
    ")\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "best_model.to_onnx('../models/resnet_branch_hybrid.onnx', dummy_input, export_params=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
