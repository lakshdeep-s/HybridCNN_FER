{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a117c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00860044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)                   # Python random module\n",
    "    np.random.seed(seed)                # NumPy random seed\n",
    "    torch.manual_seed(seed)             # PyTorch CPU seed\n",
    "    torch.cuda.manual_seed(seed)        # PyTorch CUDA seed\n",
    "    torch.cuda.manual_seed_all(seed)    # All CUDA devices if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True   # For reproducibility\n",
    "    torch.backends.cudnn.benchmark = False      # Disable for reproducibility\n",
    "\n",
    "# Example usage:\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acf8280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mean = [0.54148953, 0.42486119, 0.37428667]\n",
    "dataset_std = [0.23021227, 0.2072772, 0.1976669 ]\n",
    "\n",
    "class ExpDataset(Dataset):\n",
    "    def __init__(self, imgs_path, csv_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.imgs_path = imgs_path\n",
    "        self.csv_data = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "    def __getitem__(self, index):\n",
    "        record = self.csv_data.iloc[index]\n",
    "\n",
    "        img_name = record['image_name']\n",
    "        img_path = os.path.join(self.imgs_path, img_name+\".jpg\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transforms to the image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.Compose([\n",
    "                transforms.Resize((224, 224)).interpolation,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "            ])(img)\n",
    "        \n",
    "        label = record['expression_label']\n",
    "        return img, label\n",
    "\n",
    "imgs_path = \"../expW/origin_cleaned\"\n",
    "labels_path = \"../expW/new_label.csv\"\n",
    "\n",
    "exp_dataset = ExpDataset(imgs_path, labels_path)\n",
    "N = len(exp_dataset)\n",
    "dataset_range = range(N)\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    dataset_range,\n",
    "    train_size=0.8,\n",
    "    random_state=42,\n",
    "    stratify=exp_dataset.csv_data['expression_label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7ed8b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3521, -2.1307, -2.1307,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.1307, -2.1307,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.1307, -2.1307,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         ...,\n",
       "         [-2.3521, -2.3521, -2.3521,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.3521, -2.3521,  ..., -2.1307, -2.1307, -2.1307],\n",
       "         [-2.3521, -2.3521, -2.3521,  ..., -2.1307, -2.1307, -2.1307]],\n",
       "\n",
       "        [[-2.0497, -1.8038, -1.8038,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -1.8038, -1.8038,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -1.8038, -1.8038,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         ...,\n",
       "         [-2.0497, -2.0497, -2.0497,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -2.0497, -2.0497,  ..., -1.8038, -1.8038, -1.8038],\n",
       "         [-2.0497, -2.0497, -2.0497,  ..., -1.8038, -1.8038, -1.8038]],\n",
       "\n",
       "        [[-1.8935, -1.6356, -1.6356,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.6356, -1.6356,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.6356, -1.6356,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         ...,\n",
       "         [-1.8935, -1.8935, -1.8935,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.8935, -1.8935,  ..., -1.6356, -1.6356, -1.6356],\n",
       "         [-1.8935, -1.8935, -1.8935,  ..., -1.6356, -1.6356, -1.6356]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.25),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=2.5, translate=(0.05, 0.05), scale=(1.05, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
    "])\n",
    "\n",
    "train_dataset = ExpDataset(imgs_path, labels_path, transform=train_transforms)\n",
    "val_dataset = ExpDataset(imgs_path, labels_path, transform=val_transforms)\n",
    "\n",
    "# Final Splits\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "img, label = train_dataset[36]\n",
    "img\n",
    "# len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d8a2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "165ac8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, padding=1, dropout=False, d=0.05, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Reduce Spatial Dimenstion here by half or more by taking (stride > 1)\n",
    "        # Use the kernel size to determine what 'kind' of features to focus on, smaller kernels for finer detail but needs more stacks\n",
    "        # to achieve the perfect receptive field\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout1 = nn.Dropout2d(d) if dropout else nn.Identity()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Use stride 1 here always\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            stride=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout2 = nn.Dropout2d(d) if dropout else nn.Identity()\n",
    "\n",
    "        # Skip connection Logic\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,  # match spatial downsample\n",
    "                    padding=0,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out_intermediate = self.relu(self.bn1(self.conv1(x)))\n",
    "        out_intermediate = self.dropout1(out_intermediate)\n",
    "\n",
    "        out_intermediate = self.bn2(self.conv2(out_intermediate))\n",
    "        out_intermediate = self.dropout2(out_intermediate)\n",
    "\n",
    "        out = out_intermediate + self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        return out, out_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3f02d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1=nn.Conv2d(in_planes, in_planes//ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "    \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv1(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8, spatial_kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(spatial_kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.ca(x)\n",
    "        out = out * self.sa(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11bb3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branch1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = ResidualBlock(16, 16, stride=1, kernel_size=3, padding=1)\n",
    "        self.block2 = ResidualBlock(16, 32, stride=2, kernel_size=3, padding=1)\n",
    "        self.cbam1 = CBAM(32, ratio=4)\n",
    "        self.block3 = ResidualBlock(32, 64, stride=2, kernel_size=3, padding=1)\n",
    "        self.cbam2 = CBAM(64, ratio=8)\n",
    "        self.block4 = ResidualBlock(64, 128, stride=2, kernel_size=3, padding=1, dropout=True)\n",
    "        self.cbam3 = CBAM(128, ratio=8)\n",
    "        self.block5 = ResidualBlock(128, 128, stride=1, kernel_size=3, padding=1, dropout=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.block1(x)\n",
    "        out, _ = self.block2(out)\n",
    "        out = self.cbam1(out)\n",
    "        out, _ = self.block3(out)\n",
    "        out = self.cbam2(out)\n",
    "        out, _ = self.block4(out)\n",
    "        out = self.cbam3(out)\n",
    "        out, final_intermediate = self.block5(out)\n",
    "        return out, final_intermediate\n",
    "\n",
    "class Branch2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = ResidualBlock(16, 32, stride=2, kernel_size=5, padding=2)\n",
    "        self.cbam1 = CBAM(32, ratio=4)\n",
    "        self.block2 = ResidualBlock(32, 64, stride=2, kernel_size=5, padding=2, dropout=True)\n",
    "        self.cbam2 = CBAM(64, ratio=8)\n",
    "        self.block3 = ResidualBlock(64, 128, stride=2, kernel_size=5, padding=2, dropout=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.block1(x)\n",
    "        out = self.cbam1(out)\n",
    "        out, _ = self.block2(out)\n",
    "        out = self.cbam2(out)\n",
    "        out, final_intermediate = self.block3(out)\n",
    "        return out, final_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c883138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MildCBAM(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super().__init__()\n",
    "        self.cbam = CBAM(in_planes, ratio=16)  # mild attention\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn = self.cbam(x)\n",
    "        return x * attn\n",
    "\n",
    "class AttentionWeightedConcat(nn.Module):\n",
    "    def __init__(self, channels1, channels2):\n",
    "        super().__init__()\n",
    "        self.attn1 = MildCBAM(channels1)\n",
    "        self.attn2 = MildCBAM(channels2)\n",
    "        self.weight1 = nn.Parameter(torch.ones(1, channels1, 1, 1))\n",
    "        self.weight2 = nn.Parameter(torch.ones(1, channels2, 1, 1))\n",
    "        self.batch_norm = nn.BatchNorm2d(channels1 + channels2)\n",
    "    \n",
    "    def forward(self, feat1, feat2):\n",
    "        modulated1 = self.attn1(feat1)\n",
    "        modulated2 = self.attn2(feat2)\n",
    "        weighted1 = modulated1 * self.weight1\n",
    "        weighted2 = modulated2 * self.weight2\n",
    "        fused = torch.cat([weighted1, weighted2], dim=1)\n",
    "        fused = self.batch_norm(fused)\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ddb37256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBranchModel(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.branch1 = Branch1()  # Your previously defined branch1\n",
    "        self.branch2 = Branch2()  # Your previously defined branch2\n",
    "        self.fusion = AttentionWeightedConcat(128, 128)  # 128 channels from each branch\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared = self.initial(x)\n",
    "        b1_out, b1_intermediate = self.branch1(shared)\n",
    "        b2_out, b2_intermediate = self.branch2(shared)\n",
    "\n",
    "        if b1_out.shape[2:] != b2_out.shape[2:]:\n",
    "            b2_out = nn.functional.interpolate(b2_out, size=b1_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        fused = self.fusion(b1_out, b2_out)\n",
    "\n",
    "        return fused, b1_intermediate, b2_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc59c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch1 intermediate: torch.Size([1, 128, 14, 14])\n",
      "Branch2 intermediate: torch.Size([1, 128, 14, 14])\n",
      "Fused features: torch.Size([1, 256, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "model = ResnetBranchModel()\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "fused, b1_int, b2_int = model(x)\n",
    "\n",
    "print(f\"Branch1 intermediate: {b1_int.shape}\")\n",
    "print(f\"Branch2 intermediate: {b2_int.shape}\")\n",
    "print(f\"Fused features: {fused.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca17d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_channels_fused, input_channels_int, embed_dim=256, num_heads=8, num_layers=4, height=14, width=14):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Project each input (fused and intermediates) to a common embedding dimension\n",
    "        self.proj_fused = nn.Conv2d(input_channels_fused, embed_dim, kernel_size=1)\n",
    "        self.proj_b1_int = nn.Conv2d(input_channels_int, embed_dim, kernel_size=1)\n",
    "        self.proj_b2_int = nn.Conv2d(input_channels_int, embed_dim, kernel_size=1)\n",
    "\n",
    "        # Learnable positional embeddings for spatial information\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_dim, height, width))\n",
    "\n",
    "        # Learnable path embeddings to distinguish between fused and branch inputs\n",
    "        self.path_embed = nn.Parameter(torch.randn(3, 1, embed_dim))  # for fused, branch1 intermediate, branch2 intermediate\n",
    "\n",
    "        # Transformer encoder stack with multi-head attention\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=0.1,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, fused, b1_int, b2_int):\n",
    "        B = fused.size(0)\n",
    "\n",
    "        # Project inputs and add positional embedding\n",
    "        fused_embed = self.proj_fused(fused) + self.pos_embed\n",
    "        b1_embed = self.proj_b1_int(b1_int) + self.pos_embed\n",
    "        b2_embed = self.proj_b2_int(b2_int) + self.pos_embed\n",
    "\n",
    "        # Flatten spatial dimensions (B, C, H, W) -> (B, H*W, C)\n",
    "        def flatten_spatial(x):\n",
    "            B, C, H, W = x.shape\n",
    "            return x.flatten(2).permute(0, 2, 1)  # shape: (B, H*W, C)\n",
    "\n",
    "        fused_seq = flatten_spatial(fused_embed)\n",
    "        b1_seq = flatten_spatial(b1_embed)\n",
    "        b2_seq = flatten_spatial(b2_embed)\n",
    "\n",
    "        # Add path embeddings broadcasted across batch and sequence length\n",
    "        fused_seq = fused_seq + self.path_embed[0]\n",
    "        b1_seq = b1_seq + self.path_embed[1]\n",
    "        b2_seq = b2_seq + self.path_embed[2]\n",
    "\n",
    "        # Concatenate sequences along token dimension (B, 3*H*W, embed_dim)\n",
    "        combined_seq = torch.cat([fused_seq, b1_seq, b2_seq], dim=1)\n",
    "\n",
    "        # Transformer expects input shape (sequence length, batch, embedding dim)\n",
    "        combined_seq = combined_seq.permute(1, 0, 2)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        transformed_seq = self.transformer_encoder(combined_seq)\n",
    "\n",
    "        # Back to (batch, sequence length, embedding dim)\n",
    "        transformed_seq = transformed_seq.permute(1, 0, 2)\n",
    "\n",
    "        # Layer norm on transformer output\n",
    "        transformed_seq = self.layer_norm(transformed_seq)\n",
    "\n",
    "        return transformed_seq  # Shape: (B, 3*H*W, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBranchHybridTransformer(L.LightningModule):\n",
    "    def __init__(self, backbone, transformer, num_classes=7,\n",
    "                 lr=1e-3, weight_decay=1e-4, max_epochs=40, min_lr=1e-6):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['backbone', 'transformer'])\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.transformer = transformer\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Set classifier input size from transformer params\n",
    "        embed_dim = transformer.layer_norm.normalized_shape[0]\n",
    "        H, W = transformer.height, transformer.width\n",
    "        self.classifier = nn.Linear(embed_dim * 3 * H * W, num_classes)\n",
    "\n",
    "        # Metrics\n",
    "        self.train_acc = MulticlassAccuracy(num_classes=num_classes, average='weighted')\n",
    "        self.val_acc = MulticlassAccuracy(num_classes=num_classes, average='weighted')\n",
    "        self.train_f1 = MulticlassF1Score(num_classes=num_classes, average='weighted')\n",
    "        self.val_f1 = MulticlassF1Score(num_classes=num_classes, average='weighted')\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        fused, b1_int, b2_int = self.backbone(x)\n",
    "        transformer_out = self.transformer(fused, b1_int, b2_int)  # [B, 3*H*W, embed_dim]\n",
    "        B = transformer_out.size(0)\n",
    "        logits = self.classifier(transformer_out.view(B, -1))\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.train_f1.update(preds, y)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = self.trainer.callback_metrics.get('train_loss')\n",
    "        acc = self.train_acc.compute().item()\n",
    "        f1 = self.train_f1.compute().item()\n",
    "\n",
    "        print(f\"[Epoch {self.current_epoch}] \"\n",
    "              f\"Train Loss: {avg_loss:.4f} | \"\n",
    "              f\"Train Acc: {acc:.4f} | \"\n",
    "              f\"Train F1: {f1:.4f}\")\n",
    "\n",
    "        self.train_acc.reset()\n",
    "        self.train_f1.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc.update(preds, y)\n",
    "        self.val_f1.update(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_f1', self.val_f1, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = self.trainer.callback_metrics.get('val_loss')\n",
    "        acc = self.val_acc.compute().item()\n",
    "        f1 = self.val_f1.compute().item()\n",
    "\n",
    "        print(f\"[Epoch {self.current_epoch}] \"\n",
    "              f\"Val Loss: {avg_loss:.4f} | \"\n",
    "              f\"Val Acc: {acc:.4f} | \"\n",
    "              f\"Val F1: {f1:.4f}\")\n",
    "\n",
    "        self.val_acc.reset()\n",
    "        self.val_f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.hparams.max_epochs,\n",
    "            eta_min=self.hparams.min_lr\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            'lr_scheduler': {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a88df69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "backbone = ResnetBranchModel(in_channels=3)\n",
    "# Instantiate the MultiInputTransformerEncoder\n",
    "# Input channel details from your model:\n",
    "# - fused features channels = 256 (128 from each branch concatenated)\n",
    "# - intermediate features channels = 128 (each branch outputs 128 channels intermediate)\n",
    "transformer = MultiInputTransformerEncoder(\n",
    "    input_channels_fused=256,\n",
    "    input_channels_int=128,\n",
    "    embed_dim=256,       # embedding dimension for transformer tokens\n",
    "    num_heads=8,         # number of attention heads\n",
    "    num_layers=4,        # number of transformer encoder layers\n",
    "    height=14,           # spatial height of feature maps\n",
    "    width=14             # spatial width of feature maps\n",
    ")\n",
    "model = ResnetBranchHybridTransformer(\n",
    "    backbone=backbone,\n",
    "    transformer=transformer,\n",
    "    num_classes=7\n",
    ")\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='max',\n",
    "    save_top_k=1,\n",
    "    dirpath='../models/checkpoints_',\n",
    "    filename='best_model'\n",
    ")\n",
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=40,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    log_every_n_steps=1,\n",
    "    logger=False,\n",
    "    devices=1,\n",
    "    precision='16-mixed',\n",
    "    num_sanity_val_steps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1278276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:658: Checkpoint directory D:\\Coding\\emotion_project\\models\\checkpoints_ exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                         | Params | Mode \n",
      "---------------------------------------------------------------------\n",
      "0 | backbone    | ResnetBranchModel            | 1.4 M  | train\n",
      "1 | transformer | MultiInputTransformerEncoder | 3.3 M  | train\n",
      "2 | classifier  | Linear                       | 1.1 M  | train\n",
      "3 | train_acc   | MulticlassAccuracy           | 0      | train\n",
      "4 | val_acc     | MulticlassAccuracy           | 0      | train\n",
      "5 | train_f1    | MulticlassF1Score            | 0      | train\n",
      "6 | val_f1      | MulticlassF1Score            | 0      | train\n",
      "7 | loss_fn     | CrossEntropyLoss             | 0      | train\n",
      "---------------------------------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.329    Total estimated model params size (MB)\n",
      "225       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "d:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "d:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcedb5728b04b189b953a599875925e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bced9dc7e9044031a761c8d9ff29e812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:153\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance(data_fetcher)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:394\u001b[39m, in \u001b[36m_TrainingEpochLoop.on_advance_end\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_accumulate():\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[32m    392\u001b[39m     call._call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m.trainer, \u001b[33m\"\u001b[39m\u001b[33mon_validation_model_zero_grad\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer._logger_connector._first_loop_iter = first_loop_iter\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:152\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_iteration_done()\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m._store_dataloader_outputs()\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:295\u001b[39m, in \u001b[36m_EvaluationLoop.on_run_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer._logger_connector._evaluation_epoch_end()\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m logged_outputs, \u001b[38;5;28mself\u001b[39m._logged_outputs = \u001b[38;5;28mself\u001b[39m._logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:375\u001b[39m, in \u001b[36m_EvaluationLoop._on_evaluation_epoch_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    373\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mon_test_epoch_end\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mon_validation_epoch_end\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m call._call_callback_hooks(trainer, hook_name)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m trainer._logger_connector.on_epoch_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\emotion_project\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mResnetBranchHybridTransformer.on_validation_epoch_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     67\u001b[39m acc = \u001b[38;5;28mself\u001b[39m.val_acc.compute().item()\n\u001b[32m     68\u001b[39m f1 = \u001b[38;5;28mself\u001b[39m.val_f1.compute().item()\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.current_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal Loss: \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mavg_loss\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.val_acc.reset()\n\u001b[32m     76\u001b[39m \u001b[38;5;28mself\u001b[39m.val_f1.reset()\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_cb.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "best_model = ResnetBranchHybridTransformer.load_from_checkpoint(\n",
    "    best_model_path,\n",
    "    backbone=backbone,\n",
    "    transformer=transformer,\n",
    "    num_classes=7\n",
    ")\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "best_model.to_onnx('../models/resnet_branch_hybrid.onnx', dummy_input, export_params=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
